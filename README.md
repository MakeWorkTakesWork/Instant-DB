# ğŸš€ Instant-DB: Documents to Searchable RAG Database in Minutes

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://img.shields.io/badge/tests-pytest-green.svg)](https://github.com/MakeWorkTakesWork/Instant-DB)

Transform any collection of documents into a production-ready, searchable RAG database with semantic search capabilities. Perfect for sales teams, knowledge workers, and anyone who needs to make their documents AI-searchable.

## âš¡ What is Instant-DB?

Instant-DB takes your **existing document processing pipeline** and adds **intelligent graph memory** with advanced semantic search. Unlike simple vector databases, Instant-DB builds knowledge graphs that understand relationships, concepts, and context. In minutes, not days.

### **Input** â†’ **Output**
```
ğŸ“„ Documents (.pdf, .docx, .ppt)     â†’     ğŸ§  Intelligent Knowledge Graph
ğŸ“ Parsed Text & Metadata           â†’     ğŸ”— Entity Relationships & Concepts  
ğŸ”§ Manual Knowledge Management       â†’     âš¡ Graph-Enhanced Semantic Search
```

## ğŸ¯ Perfect For

- **ğŸ¢ SaaS Sales Teams**: Search across pitch decks, objection handling, product docs
- **ğŸ“š Knowledge Workers**: Research papers, documentation, meeting notes
- **ğŸ“ Students/Researchers**: Academic papers, study materials, personal notes
- **ğŸ’¼ Consultants**: Client materials, case studies, methodologies

## âœ¨ Key Features

### **ğŸ“Š Multiple Vector Database Support**
- **ChromaDB** - Best for most users (default)
- **FAISS** - High performance for large datasets  
- **SQLite** - Simple, portable for small collections

### **ğŸ§  Multiple Embedding Providers**
- **Sentence Transformers** - Free, runs locally
- **OpenAI Embeddings** - Premium quality, API-based

### **ğŸ”— Ready-to-Use Integrations**
- **Custom GPT** - Upload and use immediately
- **Local LLMs** - Ollama, LM Studio integration
- **API Server** - REST endpoints for custom apps
- **Export Formats** - Ready for LangChain, LlamaIndex

### **ğŸ§  Graph Memory Features**
- **Entity Extraction**: Automatically identifies people, products, metrics, concepts
- **Relationship Discovery**: Maps connections between entities across documents
- **Concept Formation**: Creates higher-level abstractions and memory clusters
- **Graph-Enhanced Search**: Finds answers using relationship reasoning
- **Context Awareness**: Understands how information relates to build insights

### **ğŸ“ˆ Production Features**
- Smart chunking with section awareness
- Relevance scoring and ranking
- Document type classification
- Metadata preservation
- Incremental updates
- Batch processing

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install sentence-transformers chromadb numpy pandas
```

### 2. Process Your Documents
```bash
# Single document
python instant_db.py process document.pdf

# Entire directory
python instant_db.py process ./sales-materials --batch

# With OpenAI embeddings (higher quality)
python instant_db.py process ./docs --embedding-provider openai
```

### 3. Search Your Knowledge Base
```bash
# Command line search
python instant_db.py search "pricing objection handling"

# Interactive search
python instant_db.py search --interactive

# Graph-enhanced search (relationship reasoning)
python instant_db.py search "pricing objections" --graph --include-relationships
```

### 4. Export for Custom GPT
```bash
# Create file ready for ChatGPT upload
python instant_db.py export --format custom-gpt
```

## ğŸ“‹ Complete Workflow Example

Starting from **raw documents** to **searchable database**:

```bash
# 1. Process documents (works with existing MegaParse outputs too!)
python instant_db.py process ./my-documents --batch

# 2. Search your knowledge base
python instant_db.py search "customer onboarding process"

# 3. Export for team use
python instant_db.py export --format custom-gpt
python instant_db.py export --format api-server

# 4. Get database stats
python instant_db.py stats
```

## ğŸ—ï¸ Architecture

```
Documents â†’ Text Extraction â†’ Smart Chunking â†’ Graph Memory Engine â†’ Intelligent Database
    â†“            â†“               â†“                    â†“                      â†“
Raw Files    Clean Text      Section-Aware      Entity Extraction      Knowledge Graph
(.pdf/.docx)  + Metadata     Chunks + Overlap   Relationships          + Vector Search
                                               Concept Formation       + Reasoning
```

## ğŸ“Š Performance Benchmarks

| Dataset Size | Processing Time | Search Time | Memory Usage |
|--------------|----------------|-------------|--------------|
| 10 documents | 2-5 minutes | <100ms | 100MB |
| 100 documents | 15-30 minutes | <200ms | 500MB |
| 1000+ documents | 1-3 hours | <300ms | 2GB |

## ğŸ” Search Quality Examples

**Query**: "How to handle pricing objections"

**Traditional keyword search** finds:
- âŒ Documents containing "pricing" AND "objections"

**Instant-DB graph-enhanced search** finds:
- âœ… "Addressing cost concerns with ROI data" (+ connected to ROI metrics)
- âœ… "When prospects have budget constraints" (+ related to budget processes)
- âœ… "Value-based selling techniques" (+ linked to product benefits)
- âœ… "Overcoming price resistance" (+ connected to competitive pricing)
- ğŸ§  **Plus relationship context**: Shows how pricing connects to products, competitors, and outcomes

## ğŸ› ï¸ Integration Options

### **ğŸ¤– Custom GPT (Easiest)**
```bash
python instant_db.py export --format custom-gpt
# Upload to https://chat.openai.com/gpts/editor
```

### **ğŸ  Local LLM (Most Private)**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2

# Use with your database
python instant_db.py serve --llm ollama
```

### **ğŸŒ OpenAI API (Best Quality)**
```python
from instant_db import InstantDB

db = InstantDB("./my_database")
context = db.search("pricing objections", top_k=3)

# Send context + question to GPT-4
response = openai.chat.completions.create(...)
```

### **ğŸ”Œ API Server (For Apps)**
```bash
python instant_db.py serve --api
# REST API available at http://localhost:5000
```

## ğŸ“ Repository Structure

```
instant-db/
â”œâ”€â”€ instant_db/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ database.py          # Vector database management
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # Embedding providers
â”‚   â”‚   â”œâ”€â”€ chunking.py          # Smart text chunking
â”‚   â”‚   â””â”€â”€ search.py            # Semantic search engine
â”‚   â”œâ”€â”€ integrations/
â”‚   â”‚   â”œâ”€â”€ custom_gpt.py        # Custom GPT export
â”‚   â”‚   â”œâ”€â”€ ollama.py            # Local LLM integration
â”‚   â”‚   â”œâ”€â”€ openai_api.py        # OpenAI API integration
â”‚   â”‚   â””â”€â”€ api_server.py        # REST API server
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ document.py          # Document processing
â”‚   â”‚   â”œâ”€â”€ megaparse.py         # MegaParse integration
â”‚   â”‚   â””â”€â”€ batch.py             # Batch processing
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â”œâ”€â”€ logging.py           # Logging utilities
â”‚       â””â”€â”€ stats.py             # Database statistics
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ sales_team_setup.py     # Sales team example
â”‚   â”œâ”€â”€ research_workflow.py    # Research workflow
â”‚   â””â”€â”€ custom_integration.py   # Custom integration example
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ instant_db.py              # Main CLI interface
â””â”€â”€ README.md
```

## ğŸ¯ Use Cases

### **Sales Team Knowledge Base**
```bash
# Process all sales materials
python instant_db.py process ./sales-collateral --batch

# Search for specific scenarios
python instant_db.py search "competitive objections"
python instant_db.py search "ROI calculations" 
python instant_db.py search "pricing strategies"

# Export for team Custom GPT
python instant_db.py export --format custom-gpt --name "Sales Assistant"
```

### **Research Paper Collection**
```bash
# Process academic papers
python instant_db.py process ./research-papers --batch --embedding-provider openai

# Advanced search with filtering
python instant_db.py search "machine learning optimization" --document-type "Research Paper"
```

### **Company Knowledge Management**
```bash
# Process internal documentation
python instant_db.py process ./company-docs --batch

# Create API for internal tools
python instant_db.py serve --api --host 0.0.0.0
```

## ğŸ”„ Works With Existing Workflows

**Already using MegaParse?** Perfect! Instant-DB works with your existing outputs:
```bash
# Process existing MegaParse outputs
python instant_db.py process-megaparse ./processed-documents
```

**Have raw documents?** We'll handle the processing:
```bash
# Full pipeline from raw documents
python instant_db.py process ./raw-documents --include-parsing
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install from source (recommended for now)
git clone https://github.com/MakeWorkTakesWork/Instant-DB.git
cd Instant-DB
pip install -e .

# Or with all optional dependencies
pip install -e ".[all]"
```

### 2. Initialize Your Project

```bash
# Create configuration and sample directories
instant-db init
```

### 3. Process Your Documents

```bash
# Single document
instant-db process document.pdf

# Entire directory 
instant-db process ./documents --batch

# With custom settings
instant-db process ./documents --batch --chunk-size 800 --workers 4
```

### 4. Search Your Knowledge Base

```bash
# Simple search
instant-db search "machine learning concepts"

# Interactive search mode
instant-db search --interactive

# Export for Custom GPT
instant-db export --format markdown --split-by-type
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

We welcome contributions! See CONTRIBUTING.md for guidelines.

---

**Transform your documents into an intelligent, searchable knowledge base in minutes, not days.** 